{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import font_manager, rc\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "font_path = \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"\n",
    "fontprop = font_manager.FontProperties(fname=font_path)\n",
    "rc('font', family=fontprop.get_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    데이터를 로드하고 ID 매핑을 처리하는 함수\n",
    "    \"\"\"\n",
    "    # 데이터 로드\n",
    "    train = pd.read_csv('/apps/study_promptengineerings/dacon/real_estate_fraud/train.csv')\n",
    "    test = pd.read_csv('/apps/study_promptengineerings/dacon/real_estate_fraud/test.csv')\n",
    "    \n",
    "    # ID 매핑 저장\n",
    "    id_mapping = pd.Series(test['ID'].values, index=test.index)\n",
    "    \n",
    "    # ID 컬럼 제거\n",
    "    train = train.drop('ID', axis=1)\n",
    "    test = test.drop('ID', axis=1)\n",
    "    \n",
    "    return train, test, id_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    결측치를 처리하는 함수\n",
    "    \"\"\"\n",
    "    # 1. 총층 처리\n",
    "    if 'train' in locals():  # 학습 데이터가 있는 경우에만 실행\n",
    "        floor_model = RandomForestRegressor(random_state=42)\n",
    "        floor_data = df[df['총층'].notnull()]\n",
    "        floor_missing = df[df['총층'].isnull()]\n",
    "        \n",
    "        floor_model.fit(floor_data[['총주차대수']], floor_data['총층'])\n",
    "        df.loc[df['총층'].isnull(), '총층'] = floor_model.predict(floor_missing[['총주차대수']])\n",
    "    \n",
    "    # 2. 욕실수 처리 - 방수 기준 최빈값\n",
    "    df['욕실수'] = df.apply(lambda row: df[df['방수'] == row['방수']]['욕실수'].mode()[0] \n",
    "                        if pd.isnull(row['욕실수']) else row['욕실수'], axis=1)\n",
    "    \n",
    "    # 3. 해당층 처리 - 총층 중간값(소숫점 올림)\n",
    "    median_floor = math.ceil(df['총층'].median())\n",
    "    df['해당층'] = df['해당층'].fillna(median_floor)\n",
    "    \n",
    "    # 4. 전용면적 처리 - 방수, 욕실수 그룹 평균\n",
    "    df['전용면적'] = df['전용면적'].fillna(\n",
    "        df.groupby(['방수', '욕실수'])['전용면적'].transform('mean')\n",
    "    )\n",
    "    \n",
    "    # 5. 총주차대수 처리\n",
    "    # 5.1 주차불가능 케이스\n",
    "    df.loc[df['총주차대수'].isnull() & (df['주차가능여부'] == '불가능'), '총주차대수'] = 0\n",
    "    \n",
    "    # 5.2 주차가능 케이스\n",
    "    parking_condition = df['총주차대수'].isnull() & (df['주차가능여부'] == '가능')\n",
    "    df['층수_대비_주차대수'] = df['총주차대수'] / df['총층']\n",
    "    mean_parking_ratio = df['층수_대비_주차대수'].mean()\n",
    "    df.loc[parking_condition, '총주차대수'] = df.loc[parking_condition, '총층'] * mean_parking_ratio\n",
    "    df = df.drop('층수_대비_주차대수', axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_features(train, test):\n",
    "    \"\"\"\n",
    "    범주형 변수 인코딩 함수\n",
    "    \"\"\"\n",
    "    # 레이블 인코딩\n",
    "    label_cols = ['중개사무소', '게재일', '제공플랫폼', '방향']\n",
    "    for col in label_cols:\n",
    "        le = LabelEncoder()\n",
    "        combined_data = pd.concat([train[col], test[col]], axis=0).astype(str)\n",
    "        le.fit(combined_data)\n",
    "        train[col] = le.transform(train[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "    \n",
    "    # 원-핫 인코딩\n",
    "    onehot_cols = ['매물확인방식', '주차가능여부']\n",
    "    onehot = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    \n",
    "    # 피처 이름 생성\n",
    "    feature_names = []\n",
    "    for col in onehot_cols:\n",
    "        unique_vals = train[col].unique()\n",
    "        feature_names.extend([f\"{col}_{val}\" for val in unique_vals])\n",
    "    \n",
    "    # 인코딩 적용\n",
    "    train_encoded = onehot.fit_transform(train[onehot_cols])\n",
    "    test_encoded = onehot.transform(test[onehot_cols])\n",
    "    \n",
    "    # 데이터프레임 결합\n",
    "    train = pd.concat([\n",
    "        train.drop(columns=onehot_cols),\n",
    "        pd.DataFrame(train_encoded, index=train.index, columns=feature_names)\n",
    "    ], axis=1)\n",
    "    \n",
    "    test = pd.concat([\n",
    "        test.drop(columns=onehot_cols),\n",
    "        pd.DataFrame(test_encoded, index=test.index, columns=feature_names)\n",
    "    ], axis=1)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def train_model(X, y, test):\n",
    "    \"\"\"\n",
    "    모델 학습 및 예측 함수\n",
    "    \"\"\"\n",
    "    # SMOTE로 데이터 균형 맞추기\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_sm, y_sm = smote.fit_resample(X, y)\n",
    "    \n",
    "    # 모델 파라미터 설정\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'class_weight': 'balanced',\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # 교차 검증 설정\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros(len(X_sm))\n",
    "    test_preds = np.zeros(len(test))\n",
    "    \n",
    "    # 모델 학습 및 예측\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_sm, y_sm)):\n",
    "        print(f\"Training fold {fold + 1}/5\")\n",
    "        \n",
    "        X_train, X_val = X_sm.iloc[train_idx], X_sm.iloc[val_idx]\n",
    "        y_train, y_val = y_sm.iloc[train_idx], y_sm.iloc[val_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[val_data],\n",
    "            num_boost_round=1000,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "                lgb.log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        oof_preds[val_idx] = model.predict(X_val)\n",
    "        test_preds += model.predict(test) / skf.n_splits\n",
    "    \n",
    "    return oof_preds, test_preds, y_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmission file has been created successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m train, test, id_mapping \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 결측치 처리\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_missing_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m test \u001b[38;5;241m=\u001b[39m handle_missing_values(test, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 피처 인코딩\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mhandle_missing_values\u001b[0;34m(df, is_train)\u001b[0m\n\u001b[1;32m     48\u001b[0m     floor_data \u001b[38;5;241m=\u001b[39m df_copy[df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총층\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()]\n\u001b[1;32m     49\u001b[0m     floor_missing \u001b[38;5;241m=\u001b[39m df_copy[df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총층\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()]\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mfloor_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfloor_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m총주차대수\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloor_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m총층\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     df_copy\u001b[38;5;241m.\u001b[39mloc[df_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총층\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misnull(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총층\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m floor_model\u001b[38;5;241m.\u001b[39mpredict(floor_missing[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m총주차대수\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# 테스트 데이터의 경우 중앙값으로 채우기\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model(oof_preds, y_sm):\n",
    "    \"\"\"\n",
    "    모델 평가 및 시각화 함수\n",
    "    \"\"\"\n",
    "    # 예측값 이진화\n",
    "    oof_preds_binary = (oof_preds > 0.5).astype(int)\n",
    "    \n",
    "    # F1 점수 계산\n",
    "    f1 = f1_score(y_sm, oof_preds_binary, average='macro')\n",
    "    print(f\"OOF F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # 혼동 행렬 시각화\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_sm, oof_preds_binary)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def main():\n",
    "    # 데이터 로드\n",
    "    train, test, id_mapping = load_data()\n",
    "    \n",
    "    # 결측치 처리\n",
    "    train = handle_missing_values(train, is_train=True)\n",
    "    test = handle_missing_values(test, is_train=False)\n",
    "    \n",
    "    # 피처 인코딩\n",
    "    train_encoded, test_encoded = encode_features(train, test)\n",
    "    \n",
    "    # 학습 데이터 준비\n",
    "    X = train_encoded.drop('허위매물여부', axis=1)\n",
    "    y = train_encoded['허위매물여부']\n",
    "    \n",
    "    # 모델 학습 및 예측\n",
    "    oof_preds, test_preds, y_sm = train_model(X, y, test_encoded)\n",
    "    \n",
    "    # 모델 평가\n",
    "    f1 = evaluate_model(oof_preds, y_sm)\n",
    "    \n",
    "    # 제출 파일 생성\n",
    "    test_preds_binary = (test_preds > 0.5).astype(int)\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': id_mapping,\n",
    "        '허위매물여부': test_preds_binary\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission file has been created successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
